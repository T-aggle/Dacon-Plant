{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7PuxXUvfY_N",
        "outputId": "15cfcbec-fd44-44ad-8f1f-556fccd2bf48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf"
      ],
      "metadata": {
        "id": "dyHC58vnqZMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore') "
      ],
      "metadata": {
        "id": "N5m6u3amfgKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "metadata": {
        "id": "Vxv3-crA7f4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CFG = {\n",
        "    'EPOCHS':20,\n",
        "    'LEARNING_RATE':1e-3,\n",
        "    'BATCH_SIZE':16,\n",
        "    'SEED':41\n",
        "}"
      ],
      "metadata": {
        "id": "1o65YmC86TDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(CFG['SEED']) # Seed 고정"
      ],
      "metadata": {
        "id": "z5YLD2576VRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_input_list = sorted(glob.glob('/content/drive/MyDrive/dacon/Pak Choi/train_input/*.csv'))\n",
        "all_target_list = sorted(glob.glob('/content/drive/MyDrive/dacon/Pak Choi/train_target/*.csv'))"
      ],
      "metadata": {
        "id": "t8buQx_M6Y_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_input_list = all_input_list[:50]\n",
        "train_target_list = all_target_list[:50]\n",
        "\n",
        "val_input_list = all_input_list[50:]\n",
        "val_target_list = all_target_list[50:]"
      ],
      "metadata": {
        "id": "uJWYc0uu6gGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, input_paths, target_paths, infer_mode):\n",
        "        self.input_paths = input_paths\n",
        "        self.target_paths = target_paths\n",
        "        self.infer_mode = infer_mode\n",
        "        \n",
        "        self.data_list = []\n",
        "        self.label_list = []\n",
        "        print('Data Pre-processing..')\n",
        "        for input_path, target_path in tqdm(zip(self.input_paths, self.target_paths)):\n",
        "            input_df = pd.read_csv(input_path)\n",
        "            target_df = pd.read_csv(target_path)\n",
        "\n",
        "            # minmax = MinMaxScaler()\n",
        "            standard = StandardScaler()\n",
        "            # norm = Normalizer()\n",
        "            # print(input_df.info())\n",
        "            input_df = input_df.drop(columns=['시간'])\n",
        "            # input_df = input_df.dropna(how='all').reset_index(drop=True)\n",
        "            # input_df = input_df.fillna(0)\n",
        "            try:\n",
        "              input_df['외부온도추정관측치'] = input_df['외부온도추정관측치'].fillna(0)\n",
        "              input_df['외부습도추정관측치'] = input_df['외부습도추정관측치'].fillna(0)\n",
        "            except:\n",
        "              input_df['외부습도관측치'] = input_df['외부습도관측치'].fillna(0)\n",
        "              input_df['외부습도관측치'] = input_df['외부습도관측치'].fillna(0)\n",
        "            imputer = KNNImputer(n_neighbors=5)\n",
        "            imputed = imputer.fit_transform(input_df)\n",
        "            input_df = pd.DataFrame(imputed, columns=input_df.columns)\n",
        "            \n",
        "            print(input_df)\n",
        "            # plt.rc('font', family='NanumBarunGothic')\n",
        "            # plt.figure(figsize=(10,5))\n",
        "            # ax = sns.heatmap(input_df)\n",
        "            # plt.show()\n",
        "            \n",
        "            # print(input_df.describe())\n",
        "            # q3 = input_df.quantile(0.75)\n",
        "            # q1 = input_df.quantile(0.25)\n",
        "\n",
        "            # iqr = (q3 - q1)\n",
        "            # iqr = iqr * 1.5\n",
        "            # lowest = q1 - iqr\n",
        "            # highest = q3 + iqr\n",
        "            # input_1 = input_df[iqr != 0.0]\n",
        "            # print(input_1)\n",
        "            # outlier_index = input_df[((input_1 < lowest) | (input_1 > highest))].index\n",
        "            # print(len(input_1))\n",
        "            # print(len(outlier_index))\n",
        "            input_df[input_df.columns] = standard.fit_transform(input_df[input_df.columns])\n",
        "            input_length = int(len(input_df)/1440)\n",
        "            target_length = int(len(target_df))\n",
        "            \n",
        "            for idx in range(target_length):\n",
        "                time_series = input_df[1440*idx:1440*(idx+1)].values\n",
        "                self.data_list.append(torch.Tensor(time_series))\n",
        "            for label in target_df[\"rate\"]:\n",
        "                self.label_list.append(label)\n",
        "        print('Done.')\n",
        "              \n",
        "    def __getitem__(self, index):\n",
        "        data = self.data_list[index]\n",
        "        label = self.label_list[index]\n",
        "        if self.infer_mode == False:\n",
        "            return data, label\n",
        "        else:\n",
        "            return data\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data_list)"
      ],
      "metadata": {
        "id": "IkUC5ATo6hrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(train_input_list, train_target_list, False)\n",
        "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
        "\n",
        "val_dataset = CustomDataset(val_input_list, val_target_list, False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
      ],
      "metadata": {
        "id": "vRfIX7O36j5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaseModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=37, hidden_size=256, batch_first=True, bidirectional=False)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256, 1),\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        hidden, _ = self.lstm(x)\n",
        "        output = self.classifier(hidden[:,-1,:])\n",
        "        return output"
      ],
      "metadata": {
        "id": "bkcmB_NX6leb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, train_loader, val_loader, scheduler, device):\n",
        "    model.to(device)\n",
        "    criterion = nn.L1Loss().to(device)\n",
        "    \n",
        "    best_loss = 9999\n",
        "    best_model = None\n",
        "    for epoch in range(1, CFG['EPOCHS']+1):\n",
        "        model.train()\n",
        "        train_loss = []\n",
        "        for X, Y in tqdm(iter(train_loader)):\n",
        "            X = X.to(device)\n",
        "            Y = Y.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            output = model(X)\n",
        "            loss = criterion(output, Y)\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss.append(loss.item())\n",
        "                    \n",
        "        val_loss = validation(model, val_loader, criterion, device)\n",
        "        \n",
        "        print(f'Train Loss : [{np.mean(train_loss):.5f}] Valid Loss : [{val_loss:.5f}]')\n",
        "        \n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "            \n",
        "        if best_loss > val_loss:\n",
        "            best_loss = val_loss\n",
        "            best_model = model\n",
        "    return best_model"
      ],
      "metadata": {
        "id": "03IMhDch6-Ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validation(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    val_loss = []\n",
        "    with torch.no_grad():\n",
        "        for X, Y in tqdm(iter(val_loader)):\n",
        "            X = X.float().to(device)\n",
        "            Y = Y.float().to(device)\n",
        "            \n",
        "            model_pred = model(X)\n",
        "            loss = criterion(model_pred, Y)\n",
        "            \n",
        "            val_loss.append(loss.item())\n",
        "            \n",
        "    return np.mean(val_loss)"
      ],
      "metadata": {
        "id": "01cIX3Pq6-cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BaseModel()\n",
        "model.eval()\n",
        "optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
        "scheduler = None\n",
        "\n",
        "best_model = train(model, optimizer, train_loader, val_loader, scheduler, device)"
      ],
      "metadata": {
        "id": "ygxDeNr9e8HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_input_list = sorted(glob.glob('/content/drive/MyDrive/dacon/Pak Choi/test_input/*.csv'))\n",
        "test_target_list = sorted(glob.glob('/content/drive/MyDrive/dacon/Pak Choi/test_target/*.csv'))"
      ],
      "metadata": {
        "id": "n2tegFVc7CtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference_per_case(model, test_loader, test_path, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    pred_list = []\n",
        "    with torch.no_grad():\n",
        "        for X in iter(test_loader):\n",
        "            X = X.float().to(device)\n",
        "            \n",
        "            model_pred = model(X)\n",
        "            \n",
        "            model_pred = model_pred.cpu().numpy().reshape(-1).tolist()\n",
        "            \n",
        "            pred_list += model_pred\n",
        "    \n",
        "    submit_df = pd.read_csv(test_path)\n",
        "    submit_df['rate'] = pred_list\n",
        "    submit_df.to_csv(test_path, index=False)"
      ],
      "metadata": {
        "id": "-VHkadtz7K8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for test_input_path, test_target_path in zip(test_input_list, test_target_list):\n",
        "    test_dataset = CustomDataset([test_input_path], [test_target_path], True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
        "    inference_per_case(best_model, test_loader, test_target_path, device)"
      ],
      "metadata": {
        "id": "qj9h7tU_7MZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "os.chdir(\"/content/drive/MyDrive/dacon/Pak Choi/test_target/\")\n",
        "submission = zipfile.ZipFile(\"/content/drive/MyDrive/dacon/Pak Choi/submission.zip\", 'w')\n",
        "for path in test_target_list:\n",
        "    path = path.split('/')[-1]\n",
        "    submission.write(path)\n",
        "submission.close()"
      ],
      "metadata": {
        "id": "qPZ8ekDj7N3d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}